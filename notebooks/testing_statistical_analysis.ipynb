{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Statistical Analysis\n",
    "## Using some of the agents as fake human reviewers.\n",
    "- Load one or more ReviewSets\n",
    "- For each Reviewer, gather the reviews and run create_ranking_vector\n",
    "- Plot the Reviewers and the (fake) human reviewers\n",
    "- Produce the heatmap comparing the reviewers\n",
    "- [next - significance analysis]\n",
    "## Plan:\n",
    "- Create 5 AnalysisPlans -> AnalysisRuns, each with 5 Hypotheses\n",
    "    - In order to run faster and use smaller LLMs, \n",
    "        - Make 5 small Datasets\n",
    "            - Use the data for assemblies of ~10 genes, review what Laura sent\n",
    "        - Make 5 Analysts\n",
    "            - brief prompts\n",
    "            - LLM:\n",
    "                - Llama 3.1 70b\n",
    "                - Gemini Flash\n",
    "                - Mixtral\n",
    "                - Haiku\n",
    "                - GPT-40-mini\n",
    "        - the Hypothesis text brief - so that the Reviewers can also be fast.\n",
    "- Create 6 Reviewers\n",
    "    - Same LLMs plus Llama 3.1 7b\n",
    "    - Three are fake humans: their names begin with H.\n",
    "    - Three are Agents: their names begin with R.\n",
    "    - Key challenge: correct formatting of ranking and summary review\n",
    "- For each AnalysisPlan, create 1 ReviewPlan using the 6 Reviewers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up parent directory and db connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/idekeradmin/Dropbox/GitHub/agent_evaluation/notebooks\n",
      "/Users/idekeradmin/Dropbox/GitHub/agent_evaluation\n",
      "['/Users/idekeradmin/Dropbox/GitHub/agent_evaluation/notebooks', '/opt/anaconda3/lib/python311.zip', '/opt/anaconda3/lib/python3.11', '/opt/anaconda3/lib/python3.11/lib-dynload', '', '/Users/idekeradmin/.local/lib/python3.11/site-packages', '/opt/anaconda3/lib/python3.11/site-packages', '/opt/anaconda3/lib/python3.11/site-packages/aeosa', '/Users/idekeradmin/Dropbox/GitHub/agent_evaluation', '/Users/idekeradmin/Dropbox/GitHub/agent_evaluation']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of the current script to the Python path\n",
    "cwd = os.getcwd()\n",
    "dirname = os.path.dirname(cwd)\n",
    "print(cwd)\n",
    "print(dirname)\n",
    "sys.path.append(dirname)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.analysis_plan import AnalysisPlan\n",
    "from services.analysisrunner import AnalysisRunner\n",
    "from models.review_plan import ReviewPlan\n",
    "from services.reviewrunner import ReviewRunner\n",
    "from app.sqlite_database import SqliteDatabase\n",
    "from app.config import load_database_config\n",
    "\n",
    "# Load the db connection details\n",
    "# db_type, uri, user, password = load_database_config(path='~/ae_config/test_config.ini')\n",
    "# self.db = Database(uri, db_type, user, password)\n",
    "\n",
    "_, database_uri, _, _ = load_database_config()\n",
    "db = SqliteDatabase(database_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate AnalysisRuns\n",
    "### Not doing this, change cell back to code if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis_plan_ids = [p1, p2, p3]\n",
    "analysis_runs = []\n",
    "for ap_id in analysis_plan_ids:\n",
    "    analysis_plan = AnalysisPlan.load(db, ap_id)\n",
    "    analysis_run = analysis_plan.generate_analysis_run()\n",
    "    analysis_runs.append(analysis_run)\n",
    "\n",
    "for analysis_run in analysis_runs:\n",
    "    runner = AnalysisRunner(db, analysis_run.object_id)\n",
    "    result = runner.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the ReviewPlans, generate the ReviewSets\n",
    "### Not doing this, change cell back to code if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review_plan_ids = [\"review_plan_ad5c1fe8-dc76-4940-a938-bdfbd569f42d\"]\n",
    "review_sets = []\n",
    "for review_plan_id in review_plan_ids:\n",
    "    review_plan = ReviewPlan.load(db, review_plan_id)\n",
    "    # Generate an empty ReviewSet from the ReviewPlan\n",
    "    review_set = review_plan.generate_review_set()\n",
    "    review_sets.append(review_set)\n",
    "\n",
    "review_sets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the ReviewSets\n",
    "### Not doing this, change cell back to code if needed\n",
    "Populate the ReviewSets with Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for review_set in review_sets:\n",
    "    # Run the ReviewSet using a ReviewRunner\n",
    "    runner = ReviewRunner(db, review_set.object_id)\n",
    "    result = runner.run()\n",
    "\n",
    "review_sets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Reviewer judgment vectors from the Reviews in the ReviewSets\n",
    "The vectors correspond to the given ordered list of ReviewSets. They are not comparable otherwise.\n",
    "\n",
    "We create a datastructure in which each Reviewer is associated with its judgment vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from typing import Dict, Tuple\n",
    "from itertools import combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.review import Review\n",
    "from itertools import combinations\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_review(db, review_set, reviewer_id):\n",
    "    for review_id in review_set.review_ids:\n",
    "        review = Review.load(db, review_id)\n",
    "        if review.agent_id == reviewer_id:\n",
    "            return review\n",
    "    raise ValueError(\"Reviewer not in ReviewSet\")\n",
    "\n",
    "def create_review_judgment_vector(db, review_set=None, reviewer_id=None):\n",
    "    \"\"\"\n",
    "    Create a judgment vector for the Review \n",
    "    belonging to the Reviewer in the ReviewSet\n",
    "\n",
    "    Each ReviewSet is the result of one or more Reviewers judging\n",
    "    (assigning a partial ranking) the Hypothesis objects in an AnalysisRun, \n",
    "    as specified in the corresponding ReviewPlan\n",
    "\n",
    "    Each Review in the ReviewSet contains a partial ranking for each Hypothesis\n",
    "    in the Review. TODO: format example. There is one Review per Reviewer.\n",
    "    \n",
    "    The Hypotheses in the Review are identified according to their order in the \n",
    "    AnalysisRun, which is in turn specified by their order in the AnalysisPlan\n",
    "\n",
    "    The vector is in N dimensions in the judgment space where\n",
    "    N is the number of unique hypothesis vs. hypothesis comparisons.\n",
    "\n",
    "    Each comparison is represented as 1 | 0 | -1 based on: \n",
    "        A > B -> 1  (prefer A)\n",
    "        A = B -> 0  (no preference)\n",
    "        A < B -> -1 (prefer B)\n",
    "    \n",
    "    :param review_set: ReviewSet object\n",
    "    :return: NumPy array representing the judgment vector\n",
    "    \"\"\"\n",
    "    review = get_review(db, review_set, reviewer_id)\n",
    "\n",
    "    ranking_data = json.loads(review.ranking_data)\n",
    "\n",
    "    N = len(ranking_data[\"ranking\"])  # Number of Hypothesis objects in the ReviewSet\n",
    "\n",
    "    hypothesis_rankings = np.zeros(N, dtype=int) \n",
    "    for hypothesis_id, ranking in ranking_data[\"ranking\"].items():\n",
    "        order = int(ranking[\"order\"])\n",
    "        stars = int(ranking[\"stars\"])\n",
    "        hypothesis_rankings [order - 1] = stars\n",
    "    \n",
    "    # Create a vector long enough to hold the unique A-B comparisons,\n",
    "    # not including self comparisons\n",
    "    review_judgment_vector_length = (N * (N - 1) // 2)\n",
    "    review_judgment_vector = np.zeros(review_judgment_vector_length, dtype=np.int8)\n",
    "    \n",
    "    idx = 0\n",
    "    for i, j in combinations(range(N), 2):\n",
    "        if  hypothesis_rankings[i] < hypothesis_rankings[j]:\n",
    "            review_judgment_vector[idx] = -1\n",
    "        elif hypothesis_rankings[i] >  hypothesis_rankings[j]:\n",
    "            review_judgment_vector[idx] = 1\n",
    "        # If the hypotheses are tied, the value remains 0\n",
    "        idx += 1\n",
    "    \n",
    "    return review_judgment_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_judgment_vector(db, review_sets=None, reviewer_id=None):\n",
    "    # For a each ReviewSet, create a judgment\n",
    "    # vector for the specified Reviewer\n",
    "    #\n",
    "    # Each judgement_vector represents coordinates\n",
    "    # in a separate set of dimensions in the\n",
    "    # judgment space. \n",
    "    #\n",
    "    # We can therefore create a merged vector\n",
    "    # by simple concatenation.\n",
    "    #\n",
    "    # NOTE\n",
    "    # The judgment vectors for the reviewers are\n",
    "    # comparable ONLY if the ReviewSets \n",
    "    # have exactly the same Reviewers. Otherwise,\n",
    "    # the vectors will be of different lengths\n",
    "    # for different reviewers and the A-B\n",
    "    # comparisons will not align.\n",
    "    judgment_vector_list = []\n",
    "    review_jvecs = {}\n",
    "    for review_set in review_sets:\n",
    "        review_jvec = create_review_judgment_vector(db, review_set=review_set, reviewer_id=reviewer_id)\n",
    "        judgment_vector_list.append(review_jvec)\n",
    "        review_jvecs[reviewer_id] = review_jvec\n",
    "    return np.concatenate(judgment_vector_list), review_jvecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making jvec for reviewer: agent_d1583c75-2d1b-441f-bc5c-78068951a4d0\n",
      "review jvecs: {'agent_d1583c75-2d1b-441f-bc5c-78068951a4d0': array([-1,  1,  1,  0,  1,  1,  1, -1, -1, -1], dtype=int8)}\n",
      "making jvec for reviewer: agent_d5b90b86-6200-410f-859f-1130c77e4dc0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Reviewer not in ReviewSet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reviewer_id \u001b[38;5;129;01min\u001b[39;00m review_plan\u001b[38;5;241m.\u001b[39magent_ids:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaking jvec for reviewer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreviewer_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m     judgment_vector, review_jvecs \u001b[38;5;241m=\u001b[39m create_judgment_vector(db, review_sets\u001b[38;5;241m=\u001b[39mreview_sets, reviewer_id\u001b[38;5;241m=\u001b[39mreviewer_id)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview jvecs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreview_jvecs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     reviewer_judgment_vectors[reviewer_id] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjudgment_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m: judgment_vector,\n\u001b[1;32m     30\u001b[0m                                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_judgment_vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m: review_jvecs}\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mcreate_judgment_vector\u001b[0;34m(db, review_sets, reviewer_id)\u001b[0m\n\u001b[1;32m     20\u001b[0m review_jvecs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review_set \u001b[38;5;129;01min\u001b[39;00m review_sets:\n\u001b[0;32m---> 22\u001b[0m     review_jvec \u001b[38;5;241m=\u001b[39m create_review_judgment_vector(db, review_set\u001b[38;5;241m=\u001b[39mreview_set, reviewer_id\u001b[38;5;241m=\u001b[39mreviewer_id)\n\u001b[1;32m     23\u001b[0m     judgment_vector_list\u001b[38;5;241m.\u001b[39mappend(review_jvec)\n\u001b[1;32m     24\u001b[0m     review_jvecs[reviewer_id] \u001b[38;5;241m=\u001b[39m review_jvec\n",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m, in \u001b[0;36mcreate_review_judgment_vector\u001b[0;34m(db, review_set, reviewer_id)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_review_judgment_vector\u001b[39m(db, review_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reviewer_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    Create a judgment vector for the Review \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    belonging to the Reviewer in the ReviewSet\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    :return: NumPy array representing the judgment vector\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     review \u001b[38;5;241m=\u001b[39m get_review(db, review_set, reviewer_id)\n\u001b[1;32m     42\u001b[0m     ranking_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(review\u001b[38;5;241m.\u001b[39mranking_data)\n\u001b[1;32m     44\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ranking_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mranking\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# Number of Hypothesis objects in the ReviewSet\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mget_review\u001b[0;34m(db, review_set, reviewer_id)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m review\u001b[38;5;241m.\u001b[39magent_id \u001b[38;5;241m==\u001b[39m reviewer_id:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m review\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReviewer not in ReviewSet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Reviewer not in ReviewSet"
     ]
    }
   ],
   "source": [
    "#from app.analysis import create_review_judgment_vector, create_judgment_vector\n",
    "from models.review_set import ReviewSet\n",
    "\n",
    "review_set_ids = [\n",
    "    # \"review_set_718d6273-a001-4576-9beb-d2bc33f661df\"\n",
    "    #\"review_set_c40dd4d1-54d2-4440-b7ce-43e8e6fccd8e\",\n",
    "    \"review_set_9951cfd0-b840-402e-acb9-b57794fd2cbb\",\n",
    "    #\"review_set_4105a7fa-8d5f-40a4-a125-e881d3310ddb\" # 3R+3H (v2) Endothelial Barrier - Set 1 ( 08.12.2024 15:15:37)\n",
    "                                                        # 3R+3H (v2) Endothelial Barrier - Set 1 - H. Sophia - review_4ec809ef-5e9d-4d9b-9967-b0e6b82e8014\n",
    "                                                        # 3R+3H (v2) Endothelial Barrier - Set 1 - H. Michelle (Cell editor) - review_5d799b3a-143e-4236-9998-b88144a13730\n",
    "                                                        # 3R+3H (v2) Endothelial Barrier - Set 1 - H. Desmond (GSR ML) - review_a52c0b88-e7f3-46c9-aee4-7146d0bf3b5a\n",
    "                                                        # 3R+3H (v2) Endothelial Barrier - Set 1 - R. Shericka (aggressive) - review_b494addc-e88e-4738-bfee-ca8442c6796b\n",
    "    ]\n",
    "review_sets = []\n",
    "for review_set_id in review_set_ids:\n",
    "    review_set = ReviewSet.load(db, review_set_id)\n",
    "    review_sets.append(review_set)\n",
    "\n",
    "reviewer_judgment_vectors = {}\n",
    "\n",
    "# We get the ReviewPlan for the first ReviewSet so that we can get the list of Reviewers\n",
    "# The list must be the same for all review sets - each Reviewer must see the same ReviewSets \n",
    "# TODO: add error checking to be sure that the Reviewers match across the ReviewSets\n",
    "review_plan = ReviewPlan.load(db, review_sets[0].review_plan_id)\n",
    "for reviewer_id in review_plan.agent_ids:\n",
    "    print(f'making jvec for reviewer: {reviewer_id}')\n",
    "    judgment_vector, review_jvecs = create_judgment_vector(db, review_sets=review_sets, reviewer_id=reviewer_id)\n",
    "    print(f\"review jvecs: {review_jvecs}\")\n",
    "    reviewer_judgment_vectors[reviewer_id] = {\"judgment_vector\": judgment_vector,\n",
    "                                               \"review_judgment_vectors\": review_jvecs}\n",
    "    \n",
    "\n",
    "reviewer_judgment_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the judgment vectors\n",
    " - Label the points with the Reviewer names\n",
    " - The Reviewer names are prefixed with \"H.\", which is stripped off before labeling\n",
    " - Agents in red squares, Humans in blue circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from typing import Dict, Tuple\n",
    "from models.agent import Agent\n",
    "\n",
    "def visualize_judgment_vectors(db,\n",
    "                               reviewer_data: Dict[str, Dict], \n",
    "                               plot_type: str = 'PCA', \n",
    "                               n_components: int = 2,\n",
    "                               random_state: int = 42,\n",
    "                               figsize: Tuple[int, int] = (10, 8)):\n",
    "    \"\"\"\n",
    "    Visualize multiple reviewers in judgment space.\n",
    "    \n",
    "    :param reviewer_data: Dictionary of reviewer data\n",
    "    :param plot_type: Type of plot ('PCA', 'UMAP', or 'TSNE')\n",
    "    :param n_components: Number of components for dimensionality reduction\n",
    "    :param random_state: Random state for reproducibility\n",
    "    :param figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Extract judgment vectors and labels\n",
    "    judgment_vectors = []\n",
    "    labels = []\n",
    "    n = 1\n",
    "    for reviewer_id, data in reviewer_data.items():\n",
    "        reviewer = Agent.load(db, reviewer_id)\n",
    "        judgment_vectors.append(data['judgment_vector'])\n",
    "        if \"label\" in data and data['label'] is not None:\n",
    "            labels.append(data['label'])\n",
    "        else:\n",
    "            labels.append(reviewer.name)\n",
    "            #labels.append(f'label#{str(n)}')\n",
    "            n += 1\n",
    "\n",
    "    # Convert to numpy array\n",
    "    all_reviewers = np.vstack(judgment_vectors)\n",
    "\n",
    "    # Determine the number of components\n",
    "    n_samples, n_features = all_reviewers.shape\n",
    "    n_components = min(n_components, n_samples, n_features)\n",
    "    \n",
    "    # Perform dimensionality reduction\n",
    "    if plot_type == 'PCA':\n",
    "        reducer = PCA(n_components=n_components, random_state=random_state)\n",
    "    elif plot_type == 'UMAP':\n",
    "        reducer = umap.UMAP(n_components=n_components, random_state=random_state)\n",
    "    elif plot_type == 'TSNE':\n",
    "        reducer = TSNE(n_components=n_components, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid plot_type. Choose 'PCA', 'UMAP', or 'TSNE'.\")\n",
    "    \n",
    "    try:\n",
    "        reduced_data = reducer.fit_transform(all_reviewers)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in dimensionality reduction: {e}\")\n",
    "        print(f\"Number of reviewers: {n_samples}, Number of features: {n_features}\")\n",
    "        return\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=figsize)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(set(labels))))\n",
    "    color_dict = {label: color for label, color in zip(set(labels), colors)}\n",
    "    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h', 'H', '+', 'x', 'd', '|', '_']\n",
    "    marker_dict = {label: marker for label, marker in zip(set(labels), markers)}\n",
    "    \n",
    "    for i, (x, y) in enumerate(reduced_data):\n",
    "        label = labels[i]\n",
    "        plt.scatter(x, y, \n",
    "                    c=[color_dict[label]], \n",
    "                    marker=marker_dict[label], \n",
    "                    label=label if label not in plt.gca().get_legend_handles_labels()[1] else \"\",\n",
    "                    alpha=0.7)\n",
    "    \n",
    "    plt.title(f\"Reviewers in {plot_type} Space\")\n",
    "    plt.xlabel(f\"{plot_type}1\")\n",
    "    plt.ylabel(f\"{plot_type}2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_judgment_vectors(db, reviewer_judgment_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Reviewer similarity heatmap\n",
    " - higher similarity is more red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "{'analyst_8148fd22-bdd8-4bda-910d-f7301f98a64d': {'judgment_vector': array([1, 1, 0], dtype=int8),\n",
    "  'review_judgment_vectors': {'analyst_8148fd22-bdd8-4bda-910d-f7301f98a64d': array([1, 1, 0], dtype=int8)}},\n",
    " 'analyst_84a2a6a3-ba70-4c9e-98ce-416aaaf5fad1': {'judgment_vector': array([-1,  1,  1], dtype=int8),\n",
    "  'review_judgment_vectors': {'analyst_84a2a6a3-ba70-4c9e-98ce-416aaaf5fad1': array([-1,  1,  1], dtype=int8)}}}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def reviewer_similarity_heatmap(db, reviewer_judgment_vectors, metric='cosine', method='average', figsize=(12, 10)):\n",
    "    \"\"\"\n",
    "    Create a similarity heatmap for judgment vectors with clustering.\n",
    "    \n",
    "    :param reviewer_judgment_vectors: dict where the keys are reviewer ids and the values are numpy vectors\n",
    "    :param judge_labels: List of labels for each reviewer\n",
    "    :param metric: Distance metric for similarity (default: 'cosine')\n",
    "    :param method: Linkage method for hierarchical clustering (default: 'average')\n",
    "    :param figsize: Figure size (width, height) in inches\n",
    "    \"\"\"\n",
    "\n",
    "    # 2D array where each row is a reviewer's vector\n",
    "    judgment_vectors = []\n",
    "    # List of reviewer labels\n",
    "    reviewer_labels = []\n",
    "\n",
    "    for reviewer_id, judgement_vector in reviewer_judgment_vectors.items():\n",
    "        reviewer = Agent.load(db, reviewer_id)\n",
    "        judgment_vectors.append(judgement_vector[\"judgment_vector\"])\n",
    "        reviewer_labels.append(reviewer.name)\n",
    "\n",
    "    # Compute pairwise distances\n",
    "    distances = pdist(judgment_vectors, metric=metric)\n",
    "    similarity_matrix = 1 - squareform(distances)  # Convert distances to similarities\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    linkage = hierarchy.linkage(distances, method=method)\n",
    "    \n",
    "    # Create a clustered heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.set_theme(font_scale=0.8)\n",
    "    \n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(h_neg=220, h_pos=10, as_cmap=True)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    g = sns.clustermap(similarity_matrix,\n",
    "                       row_linkage=linkage,\n",
    "                       col_linkage=linkage,\n",
    "                       cmap=cmap,\n",
    "                       center=0,\n",
    "                       annot=True,\n",
    "                       fmt='.2f',\n",
    "                       xticklabels=reviewer_labels,\n",
    "                       yticklabels=reviewer_labels,\n",
    "                       figsize=figsize)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.setp(g.ax_heatmap.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.title(\"Reviewer Similarity Heatmap\", pad=50)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from app.analysis import reviewer_similarity_heatmap\n",
    "\n",
    "reviewer_similarity_heatmap(db, reviewer_judgment_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
