{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for creating agents programatically \n",
    "\n",
    "### set up parent directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/idekeradmin/Dropbox/GitHub/agent_evaluation/notebooks\n",
      "/Users/idekeradmin/Dropbox/GitHub/agent_evaluation\n",
      "['/Users/idekeradmin/Dropbox/GitHub/agent_evaluation/notebooks', '/opt/anaconda3/lib/python311.zip', '/opt/anaconda3/lib/python3.11', '/opt/anaconda3/lib/python3.11/lib-dynload', '', '/Users/idekeradmin/.local/lib/python3.11/site-packages', '/opt/anaconda3/lib/python3.11/site-packages', '/opt/anaconda3/lib/python3.11/site-packages/aeosa', '/Users/idekeradmin/Dropbox/GitHub/agent_evaluation']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of the current script to the Python path\n",
    "cwd = os.getcwd()\n",
    "dirname = os.path.dirname(cwd)\n",
    "print(cwd)\n",
    "print(dirname)\n",
    "sys.path.append(dirname)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load database \n",
    "\n",
    "- make sure there is a ~/ae_config/config.ini file for all the configs, and ~/ae_database/ae_database.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.analysis_plan import AnalysisPlan\n",
    "from services.analysisrunner import AnalysisRunner\n",
    "from app.sqlite_database import SqliteDatabase\n",
    "from app.config import load_database_config\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load the db connection details\n",
    "# db_type, uri, user, password = load_database_config(path='~/ae_config/test_config.ini')\n",
    "# self.db = Database(uri, db_type, user, password)\n",
    "\n",
    "_, database_uri, _, _ = load_database_config()\n",
    "db = SqliteDatabase(database_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the available LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Levar the Llama - 3.1-70b-versitile': 'llm_a38b5b94-14d6-4216-b166-6f1515faa3a1',\n",
       " 'Holly the Haiku 20240307': 'llm_23a5c740-4d9b-4839-8c0a-7fb6ccae1ff1',\n",
       " 'Claudia the claude-3-5-sonnet-20240620': 'llm_bb90988c-fa06-4a3a-99cc-81608fa8d1d7',\n",
       " 'Gary the GPT-4o-mini 2024-07-08': 'llm_09620189-3486-4118-941c-7c674bc25657',\n",
       " 'Maxine the mixtral-8x7b-32768': 'llm_6e44f8e2-5089-40b1-ad37-db18f31f2b39',\n",
       " 'Georgia the gpt-4o-2024-05-13': 'llm_dc39023b-77bf-4d2f-a6a0-829ce9c3655d'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_specs = db.find(\"llm\")\n",
    "llm_mappings = {}\n",
    "for llm_spec in llm_specs:\n",
    "    llm_id = llm_spec[\"object_id\"]\n",
    "    llm_properties = llm_spec[\"properties\"]\n",
    "    llm_name = llm_properties[\"name\"]\n",
    "    llm_mappings[llm_name] = llm_id\n",
    "\n",
    "llm_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyst_brief_0814\n",
      "context_gsr_1st_yr_ml_0814\n",
      "reviewer_reflect_0814\n",
      "context_gsr_0814\n",
      "reviewer_0814\n",
      "reviewer_cot_reflect_0814\n",
      "analyst_cot_brief_0814\n",
      "reviewer_cot_0814\n",
      "context_pi_0814\n",
      "analyst_contrary_kg_brief_0814\n",
      "context_cell_editor\n",
      "analyst_minimal_0814\n",
      "reviewer_cot_reflect_novelty_0814\n",
      "analyst_kg_brief_0814\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "dirname = os.path.dirname(cwd)\n",
    "\n",
    "prompt_directory = (os.path.join(dirname, \"prompts\"))\n",
    "\n",
    "def read_text_files(directory):\n",
    "    file_contents = {}\n",
    "    \n",
    "    # Iterate through all files in the given directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file is a text file\n",
    "        if filename.endswith('.txt'):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            # Get the filename without extension\n",
    "            file_key = os.path.splitext(filename)[0]\n",
    "            \n",
    "            # Read the contents of the file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    file_contents[file_key] = content\n",
    "            except IOError as e:\n",
    "                print(f\"Error reading file {filename}: {e}\")\n",
    "    \n",
    "    return file_contents\n",
    "\n",
    "prompts = read_text_files(prompt_directory)\n",
    "\n",
    "for name, text in prompts.items():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Analyst Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C. Marvin': <models.agent.Agent at 0x166c018d0>,\n",
       " 'A. Boris': <models.agent.Agent at 0x166c01b10>,\n",
       " 'A. Alex': <models.agent.Agent at 0x166c01d50>,\n",
       " 'A. Jane': <models.agent.Agent at 0x166c01f50>,\n",
       " 'A. Gloria': <models.agent.Agent at 0x166c02150>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.agent import Agent\n",
    "\n",
    "llm_claude_3_5 = llm_mappings[\"Claudia the claude-3-5-sonnet-20240620\"]\n",
    "\n",
    "analyst_specs = {\n",
    "    \"C. Marvin\":{\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"analyst_contrary_kg_brief_0814\"],\n",
    "        \"context\": prompts[\"context_gsr_0814\"],\n",
    "        \"description\": \"Contrarian agent with knowledge graph. It provides a kind of guardrail. The expectation would be that it's hypotheses will typically be ranked lowest when the data is reasonably cohesive but should be picked first by the judge ('yes, I agree, there is no signal here') in a control case where the data and gene selection is random.\"\n",
    "    },\n",
    "    \"A. Boris\":{\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"analyst_minimal_0814\"],\n",
    "        \"context\": prompts[\"context_gsr_0814\"],\n",
    "        \"description\": \"grad student agent with a minimal prompt\"\n",
    "    },\n",
    "    \"A. Alex\": {\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"analyst_brief_0814\"],\n",
    "        \"context\": prompts[\"context_gsr_0814\"],\n",
    "        \"description\": \"grad student agent\"\n",
    "    },\n",
    "    \"A. Jane\":{\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"analyst_kg_brief_0814\"],\n",
    "        \"context\": prompts[\"context_gsr_0814\"],\n",
    "        \"description\": \"grad student agent constructing a knowledge graph\"\n",
    "    },\n",
    "    \"A. Gloria\":{\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"analyst_cot_brief_0814\"],\n",
    "        \"context\": prompts[\"context_gsr_0814\"],\n",
    "        \"description\": \"grad student agent using chain of thought\"\n",
    "    }\n",
    "}\n",
    "\n",
    "analysts = {}\n",
    "\n",
    "for name, spec in analyst_specs.items():\n",
    "    analysts[name] = Agent.create(db, \n",
    "                                  spec[\"llm_id\"], \n",
    "                                  spec[\"context\"],\n",
    "                                  spec[\"prompt_template\"],\n",
    "                                  name=name,\n",
    "                                  description=spec.get('description'))\n",
    "    \n",
    "analysts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Reviewer Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R. Wei': <models.agent.Agent at 0x16ae9bed0>,\n",
       " 'R. Ricardo': <models.agent.Agent at 0x168086c90>,\n",
       " 'R. Frederico': <models.agent.Agent at 0x1053494d0>,\n",
       " 'H. Sophia': <models.agent.Agent at 0x105349310>,\n",
       " 'H. Michelle': <models.agent.Agent at 0x105349150>,\n",
       " 'H. Sanjay': <models.agent.Agent at 0x16cb98050>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from models.agent import Agent\n",
    "\n",
    "llm_claude_3_5 = llm_mappings[\"Claudia the claude-3-5-sonnet-20240620\"]\n",
    "\n",
    "pi_context = prompts[\"context_pi_0814\"]\n",
    "reviewer_specs = {\n",
    "    \"R. Wei\":{\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"reviewer_cot_reflect_novelty_0814\"],\n",
    "        \"context\": pi_context,\n",
    "        \"description\": \"PI reviewer preferring novelty, chain of thought, and reflection\"\n",
    "    },\n",
    "    \"R. Ricardo\":{\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"reviewer_cot_reflect_0814\"],\n",
    "        \"context\": pi_context,\n",
    "        \"description\": \"PI reviewer using chain of thought and reflection\"\n",
    "    },\n",
    "    \"R. Frederico\": {\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"reviewer_cot_0814\"],\n",
    "        \"context\": pi_context,\n",
    "        \"description\": \"PI reviewer using chain of thought\"\n",
    "    },\n",
    "    \"H. Sophia\":{\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"reviewer_reflect_0814\"],\n",
    "        \"context\": pi_context,\n",
    "        \"description\": \"PI reviewer using reflection\"\n",
    "    },\n",
    "    \"H. Michelle\":{\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"reviewer_0814\"],\n",
    "        \"context\": pi_context,\n",
    "        \"description\": \"PI reviewer\"\n",
    "    },\n",
    "    \"H. Sanjay\":{\n",
    "        \"llm_id\": llm_claude_3_5,\n",
    "        \"prompt_template\": prompts[\"reviewer_0814\"],\n",
    "        \"context\": prompts[\"context_cell_editor\"],\n",
    "        \"description\": \"Cell reviewer\"\n",
    "    }\n",
    "}\n",
    "\n",
    "reviewers = {}\n",
    "\n",
    "for name, spec in reviewer_specs.items():\n",
    "    reviewers[name] = Agent.create(db, \n",
    "                                  spec[\"llm_id\"], \n",
    "                                  spec[\"context\"],\n",
    "                                  spec[\"prompt_template\"],\n",
    "                                  name=name,\n",
    "                                  description=spec.get('description'))\n",
    "    \n",
    "reviewers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new analysis plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new analysis run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nanalysis_run = analysis_plan.generate_analysis_run(biological_context=\"Chromatin remodelling and transcriptional regulation (or Transcriptional regulation and chromatin remodelling)\")\\nprint(analysis_run.object_id)\\nprint(vars(analysis_run))\\n\\nrunner = AnalysisRunner(db, analysis_run.object_id)\\nresult = runner.run()\\nprint(result)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "analysis_run = analysis_plan.generate_analysis_run(biological_context=\"Chromatin remodelling and transcriptional regulation (or Transcriptional regulation and chromatin remodelling)\")\n",
    "print(analysis_run.object_id)\n",
    "print(vars(analysis_run))\n",
    "\n",
    "runner = AnalysisRunner(db, analysis_run.object_id)\n",
    "result = runner.run()\n",
    "print(result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nhypotheses = db.load(analysis_run.object_id)[0]['hypothesis_ids']\\nhypotheses\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "hypotheses = db.load(analysis_run.object_id)[0]['hypothesis_ids']\n",
    "hypotheses\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom models.hypothesis import Hypothesis\\n%reload_ext autoreload\\n%autoreload 2\\n\\nfor hypothesis_id in hypotheses:\\n    hypothesis = Hypothesis.load(db, hypothesis_id)\\n    # print(hypothesis_id)\\n    print(hypothesis.object_id)\\n    print(hypothesis.hypothesis_text)\\n    print(hypothesis.full_prompt)\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from models.hypothesis import Hypothesis\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "for hypothesis_id in hypotheses:\n",
    "    hypothesis = Hypothesis.load(db, hypothesis_id)\n",
    "    # print(hypothesis_id)\n",
    "    print(hypothesis.object_id)\n",
    "    print(hypothesis.hypothesis_text)\n",
    "    print(hypothesis.full_prompt)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
