<experiment_description>
{experiment_description}
</experiment_description>

<biological_context>
{biological_context}
</biological_context>

<dataset>
{data}
</dataset>

<hypotheses>
{hypotheses_text}
</hypotheses>

<instructions>
<task>
Review and rank hypotheses explaining experimental data based on these criteria:
1. Accurate use of data
2. Accurate use of scientific knowledge
3. Correct reasoning
4. Based on causal mechanisms explaining data
5. Plausible
6. Novel
7. Relevant to experiment goals
8. Actionable

Hypotheses failing accuracy or reasoning criteria are unacceptable.
</task>

<review_process>
1. Review each hypothesis, noting merits relative to criteria. Mark unacceptable ones.

<example>
Initial Review:
Hypothesis#1
...brief notes
Hypothesis#2
...brief notes
(and so on)
</example>

2. Rank hypotheses (minimum 1, maximum = number of hypotheses). Use same rank for equal preference. 
Unacceptable hypotheses get rank 1. You must output your rankings in EXACTLY format in the following example:

<example>
Initial Rankings:
Hypothesis#1: 3
Hypothesis#2: 2
...and so on
</example>

3. Perform a second review, challenging your first review. Be aware of cognitive biases.

<example>
Second Review:
Hypothesis#1
...brief notes
Hypothesis#2
...brief notes
(and so on)
</example>

4. Output revised rankings. You must output your rankings in EXACTLY format in the following example:

<example>
Final Rankings:
Hypothesis#1: 3
Hypothesis#2: 2
...and so on
</example>

5. Provide a summary review explaining your rankings. You must output the Summary Review exactly as in the following example.

<example>
Summary Review:
After careful review, I ranked Hypothesis #1 as best (rank=1), while hypotheses #2 and #3 are tied (rank=3).

Hypothesis #1 is best because:
[Brief explanation of strengths]

In comparison, Hypotheses #2 and #3 [Brief comparison]
</example>

</review_process>
</instructions>